---
id: ocolecionadortraining-advanced
title: OColecionadorTraining - Fluxos AvanÃ§ados
sidebar_label: AvanÃ§ado
---

# OColecionadorTraining - Fluxos AvanÃ§ados

DocumentaÃ§Ã£o detalhada dos fluxos complexos de treinamento e otimizaÃ§Ãµes.

---

## ðŸ”„ Fluxo 1: Pipeline Completo de Treinamento com Duas Fases

```mermaid
sequenceDiagram
    participant RabbitMQ as ðŸ“¨ RabbitMQ
    participant Training as ðŸŽ“ Training Service
    participant DataLoader as ðŸ“Š Data Loader
    participant ModelBuilder as ðŸ§  Model Builder
    participant Phase1 as ðŸ”’ Phase 1: Extract
    participant Phase2 as ðŸ”“ Phase 2: Fine-tune
    participant Callbacks as ðŸ“ˆ Callbacks
    participant MinIO as ðŸ“¦ MinIO
    participant Database as ðŸ’¾ PostgreSQL

    RabbitMQ->>Training: Delivery (batchId, totalImages)
    Training->>Database: INSERT training_jobs (status: training)
    
    Training->>MinIO: Download dataset (9000 imagens)
    MinIO-->>DataLoader: Stream de imagens
    
    DataLoader->>DataLoader: Split train 80% val 20%
    DataLoader->>DataLoader: Cria generators com augmentaÃ§Ã£o
    DataLoader-->>Training: train_gen, val_gen
    
    Training->>ModelBuilder: ConstrÃ³i MobileNetV2 base
    ModelBuilder->>ModelBuilder: Load imagenet weights
    ModelBuilder->>ModelBuilder: Congela base
    ModelBuilder->>ModelBuilder: Adiciona top layers
    ModelBuilder-->>Training: Model ready
    
    Training->>Training: Compila com Adam, LR=0.001
    Training->>Phase1: 10 epochs (base congelado)
    
    loop Epoch 1-10
        Phase1->>Phase1: Forward pass
        Phase1->>Phase1: Backward pass
        Phase1->>Phase1: Update top layers
        Phase1->>Callbacks: Log metrics
        Phase1->>Database: INSERT training_metrics
    end
    
    Phase1-->>Phase2: Modelo com top layers treinado
    
    Training->>Phase2: Descongela Ãºltimas 50 layers
    Training->>Training: Recompila com LR=0.00001
    Training->>Phase2: 20 epochs (fine-tuning)
    
    loop Epoch 11-30
        Phase2->>Phase2: Forward pass
        Phase2->>Phase2: Backward pass
        Phase2->>Phase2: Update all layers (low LR)
        Phase2->>Callbacks: Early Stopping check
        Phase2->>Callbacks: Reduce LR on plateau
        Phase2->>Database: INSERT training_metrics
    end
    
    Phase2-->>Training: Modelo treinado
    Training->>Training: Avaliar no test set
    Training->>Training: Calcula accuracy, precision, recall, f1
    
    Training->>Training: Salva model.h5 e weights.h5
    Training->>MinIO: Upload modelo versionado
    MinIO-->>Training: OK
    
    Training->>Database: UPDATE training_jobs (status: success)
    Training->>RabbitMQ: Publish (ModelEvaluation queue)
```

---

## ðŸ“Š Fluxo 2: Data Pipeline com AugmentaÃ§Ã£o em Tempo Real

```mermaid
graph LR
    A["ðŸ“¦ Dataset MinIO<br/>9000 imagens raw"] -->|Download| B["ðŸ’¾ Local Storage"]
    
    B -->|Split| B1["Train 80%<br/>7200 imagens"]
    B -->|Split| B2["Val 20%<br/>1800 imagens"]
    
    B1 -->|Batch 1-32| C["ðŸ”„ ImageDataGenerator"]
    
    C -->|Pipeline| D["1. Resize 224x224"]
    D -->|Pipeline| E["2. Rotation Â±20Â°"]
    E -->|Pipeline| F["3. Width shift Â±20%"]
    F -->|Pipeline| G["4. Height shift Â±20%"]
    G -->|Pipeline| H["5. Horizontal flip"]
    H -->|Pipeline| I["6. Zoom Â±20%"]
    I -->|Pipeline| J["7. Normalize 0-1"]
    
    J -->|Generator output| K["ðŸ§  Model Input<br/>Batch 32x224x224x3"]
    
    B2 -->|Validation| L["Val Pipeline<br/>Apenas resize + normalize"]
    
    style A fill:#fff3e0
    style C fill:#e1f5ff
    style K fill:#f3e5f5
```

---

## ðŸ”„ Fluxo 3: Early Stopping e Learning Rate Scheduling

```mermaid
sequenceDiagram
    participant Training as ðŸŽ“ Training
    participant Optimizer as âš™ï¸ Optimizer
    participant EarlyStopping as ðŸ›‘ Early Stopping
    participant ReduceLR as ðŸ“‰ Reduce LR
    participant Database as ðŸ’¾ Database

    loop Each Epoch
        Training->>Training: train_gen.fit_on_batch()
        Training->>Training: val_gen.evaluate()
        
        Training->>Database: val_loss, val_accuracy
        
        Training->>EarlyStopping: Check patience
        alt val_loss improved
            EarlyStopping->>EarlyStopping: Reset patience counter
            EarlyStopping->>Training: Save best weights
        else val_loss NOT improved
            EarlyStopping->>EarlyStopping: patience++
            alt patience >= 5
                EarlyStopping->>Training: STOP TRAINING
                Training->>Training: Restore best weights
            end
        end
        
        Training->>ReduceLR: Check val_loss trend
        alt val_loss plateaued
            ReduceLR->>Optimizer: LR = LR * 0.1
            ReduceLR->>Database: Log LR reduction
        end
    end
```

---

## ðŸ” Fluxo 4: ValidaÃ§Ã£o de Qualidade do Modelo

```mermaid
sequenceDiagram
    participant Training as ðŸŽ“ Training
    participant Validator as âœ“ Validator
    participant Metrics as ðŸ“Š Metrics Calculator
    participant Database as ðŸ’¾ Database
    participant Monitor as ðŸš¨ Monitor

    Training->>Validator: Model training complete
    
    Validator->>Metrics: Calcula accuracy
    Validator->>Metrics: Calcula precision per class
    Validator->>Metrics: Calcula recall per class
    Validator->>Metrics: Calcula F1 score
    
    Metrics-->>Validator: MÃ©tricas calculadas
    
    Validator->>Validator: Check accuracy gt 0.90
    alt Accuracy lt 0.90
        Validator-->>Monitor: Model rejected
        Monitor->>Database: Flag quality issue
    else Accuracy OK
        Validator->>Validator: Check precision gt 0.88
    end
    
    alt Precision lt 0.88
        Validator-->>Monitor: Model rejected
    else Precision OK
        Validator->>Validator: Check recall gt 0.88
    end
    
    alt Recall lt 0.88
        Validator-->>Monitor: Model rejected
    else All metrics OK
        Validator->>Database: Mark model as APPROVED
        Validator->>Database: Ready for production
    end
```

---

## ðŸš€ Fluxo 5: Escalabilidade com Distributed Training

```mermaid
graph TB
    subgraph DataParallel["Data Parallel Training"]
        A["Batch 1024<br/>imagens"]
        A -->|Split| A1["GPU 0<br/>Batch 256"]
        A -->|Split| A2["GPU 1<br/>Batch 256"]
        A -->|Split| A3["GPU 2<br/>Batch 256"]
        A -->|Split| A4["GPU 3<br/>Batch 256"]
        
        A1 -->|Forward| B1["Loss 0"]
        A2 -->|Forward| B2["Loss 1"]
        A3 -->|Forward| B3["Loss 2"]
        A4 -->|Forward| B4["Loss 3"]
        
        B1 -->|Average| C["Mean Loss"]
        B2 -->|Average| C
        B3 -->|Average| C
        B4 -->|Average| C
        
        C -->|Backward| D1["Gradient 0"]
        C -->|Backward| D2["Gradient 1"]
        C -->|Backward| D3["Gradient 2"]
        C -->|Backward| D4["Gradient 3"]
        
        D1 -->|AllReduce| E["Gradients Averaged"]
        D2 -->|AllReduce| E
        D3 -->|AllReduce| E
        D4 -->|AllReduce| E
        
        E -->|Update| F["Weights sync"]
    end
    
    style A fill:#fff3e0
    style A1 fill:#e1f5ff
    style A2 fill:#e1f5ff
    style A3 fill:#e1f5ff
    style A4 fill:#e1f5ff
    style C fill:#f3e5f5
    style F fill:#e8f5e9
```

---

## ðŸ“Š Fluxo 6: Benchmark - Timeline de Treinamento

```mermaid
graph LR
    A["Start<br/>t=0s"] -->|Data load<br/>120s| B["Generators ready<br/>t=120s"]
    B -->|Model build<br/>10s| C["Model compiled<br/>t=130s"]
    C -->|Phase 1<br/>Feature Extraction<br/>10 epochs x 180s| D["Phase 1 done<br/>t=1930s"]
    D -->|Phase 2<br/>Fine-tuning<br/>20 epochs x 200s| E["Phase 2 done<br/>t=5930s"]
    E -->|Evaluation<br/>30s| F["Metrics calc<br/>t=5960s"]
    F -->|Save + Upload<br/>60s| G["Complete<br/>t=6020s"]
    
    G -->|Total| H["~100 minutes"]
    
    style A fill:#fff3e0
    style H fill:#e8f5e9
```

---

## ðŸ’¾ Fluxo 7: PersistÃªncia Completa em PostgreSQL

```mermaid
sequenceDiagram
    participant Training as ðŸŽ“ Training
    participant Jobs as ðŸ’¾ training_jobs
    participant Metrics as ðŸ’¾ training_metrics
    participant Hyperparams as ðŸ’¾ hyperparameters
    participant Models as ðŸ’¾ model_versions

    Training->>Jobs: INSERT (status: training)
    Jobs-->>Training: job_id = 1
    
    loop Each Epoch
        Training->>Metrics: INSERT epoch metrics
        Metrics->>Metrics: loss, accuracy, val_loss, val_accuracy
        Training->>Training: Check early stopping
    end
    
    Training->>Hyperparams: INSERT final params
    Hyperparams->>Hyperparams: batch_size, lr, optimizer, etc
    
    Training->>Training: Training complete
    Training->>Jobs: UPDATE status = success
    Training->>Jobs: UPDATE accuracy = 0.9412
    Training->>Jobs: UPDATE training_time = 3600
    
    Training->>Models: INSERT new model version
    Models->>Models: model_version, accuracy, precision, recall
    Models->>Models: model_path, trained_at
```

---

## ðŸŽ¯ ConclusÃ£o

O **OColecionadorTraining** oferece:

âœ… **Two-Phase Training** â€“ Feature extraction + Fine-tuning  
âœ… **Data Augmentation** â€“ Em tempo real durante treino  
âœ… **Early Stopping** â€“ Para evitar overfitting  
âœ… **Learning Rate Scheduling** â€“ OtimizaÃ§Ã£o adaptativa  
âœ… **Checkpoint Management** â€“ Salva best model  
âœ… **Hyperparameter Tuning** â€“ Grid search suportado  
âœ… **Cross-Validation** â€“ K-folds para robustez  
âœ… **Distributed Training** â€“ Multi-GPU support  
âœ… **Full Observability** â€“ MÃ©tricas em tempo real  
âœ… **Production Ready** â€“ Versionamento e tracking  

**Tempo mÃ©dio: 60-120 minutos por modelo**  
**AcurÃ¡cia final: 92-96%**